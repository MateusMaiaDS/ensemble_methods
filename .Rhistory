gain[i]<-  pull(data,attributes[i]) %>% entropy
}
entropy<-function(data_vector){
total<-length(data_vector)
each_class<-levels(data_vector)
summary<-table(data_vector) %>% as.data.frame()
summary$Freq<-summary$Freq/total
entropy<- -sum(summary$Freq*log2(summary$Freq))
return(entropy)
}
for(i in length(attributes)){
gain[i]<-  pull(data,attributes[i]) %>% entropy
}
gain
attributes
data_vector<-pull(data,"temp")
total<-length(data_vector)
each_class<-levels(data_vector)
summary<-table(data_vector) %>% as.data.frame()
summary
summary$Freq<-summary$Freq/total
summary
entropy<- -sum(summary$Freq*log2(summary$Freq))
entropy
summary$Freq*log2(summary$Freq)
sum(summary$Freq*log2(summary$Freq))
entropy<- -sum(summary$Freq*log2(summary$Freq))
entropy
log2(summary$Freq)
ga<-function(data, classe){
niveis<-levels(data)
resp<-c()
for(i in 1:length(niveis)){
sv<-table(classe[which(data==niveis[i])])
sv<-subset(sv, sv!=0)
if(length(sv)>0)
resp[i]<-entropia(sv)*(sum(sv)/length(data))
}
resp<-entropia(table(classe))-sum(resp)
resp
}
ga
ga(data=data,classe='play')
ga(data=data,classe=data$play)
total<-length(data_vector)
each_class<-levels(data_vector)
summary<-table(data_vector) %>% as.data.frame()
summary$Freq<-summary$Freq/total
entropy<- -sum(summary$Freq*log2(summary$Freq))
entropy
gains<-c()
gain<-c()
for(i in length(attributes)){
gain[i]<-  pull(data,attributes[i]) %>% entropy
}
entropy<-function(data_vector){
total<-length(data_vector)
each_class<-levels(data_vector)
summary<-table(data_vector) %>% as.data.frame()
summary$Freq<-summary$Freq/total
entropy<- -sum(summary$Freq*log2(summary$Freq))
return(entropy)
}
for(i in length(attributes)){
gain[i]<-  pull(data,attributes[i]) %>% entropy
}
gain
gain<-c()
for(i in length(attributes)){
gain[i]<-  entropy(pull(data,attributes[i]))
}
i=1
pull(data,attributes[i])
entropy(pull(data,attributes[i]))
gain<-numeric(0)
for(i in length(attributes)){
gain[i]<-  entropy(pull(data,attributes[i]))
}
gain<-numeric(0)
for(i in length(attributes)){
gain[i]<-  entropy(pull(data,attributes[i]))
}
i=1
entropy(pull(data,attributes[i]))
gain[i]<-  entropy(pull(data,attributes[i]))
for(i in 1:length(attributes)){
gain[i]<-  entropy(pull(data,attributes[i]))
}
gain[i]<- entropy(pull(data,class))-  entropy(pull(data,attributes[i]))
for(i in 1:length(attributes)){
gain[i]<- entropy(pull(data,class))-  entropy(pull(data,attributes[i]))
}
which.max(gain)
gain<-numeric(0)
for(i in 1:length(attributes)){
gain[i]<- entropy(pull(data,class))-  entropy(pull(data,attributes[i]))
}
attributes
data
entropia(data$play)
entropia<-function(data){
resp<-data/sum(data)
resp<-sum(resp*log2(resp))*(-1)
resp
}
entropia<-function(data){
resp<-data/sum(data)
resp<-sum(resp*log2(resp))*(-1)
resp
}
entropia(data$play)
data$play %>% summary
-5/14*log2(5/14)-9/14*log2(9/14)
entropy(data$play)
entropy(pull(data,attributes[i]))
attributes
entropy(pull(data,"wind")
)
data$wind %>% table
group_by(data,attributes) %>% summarise(entropy=entropy(class))
group_by(data,attributes[i]) %>% summarise(entropy=entropy(class))
i
group_by(data,attributes[i])
class
group_by(data,attributes[i]) %>% summarise(x=entropy(class))
roup_by(data,attributes[i]) %>% summarise(x=unique(class))
group_by(data,attributes[i]) %>% summarise(x=unique(class))
group_by(data,attributes[i]) %>% summarise(x=unique(play))
group_by(data,attributes[i]) %>% mutate(x=entropy(play)
)
group_by(data,attributes[1]) %>% mutate(x=entropy(play))
group_by(data,attributes[1]) %>% summarise(x=entropy(play))
rm(list=ls())
log(4/2)
#read_csv1('bag_words_bbc_bs_new.csv')
bag_words_bbc_bs_new <- read_csv("D:/my_computer/Est ML 2018_complete/CSDS/bag_words_bbc_bs_new.csv")
library(tidyverse)
#read_csv1('bag_words_bbc_bs_new.csv')
bag_words_bbc_bs_new <- read_csv("D:/my_computer/Est ML 2018_complete/CSDS/bag_words_bbc_bs_new.csv")
bag_words_bbc_bs_new<-bag_words_bbc_bs_new%>%
mutate(news_class=news_class%>%as.factor)
banco_de_dados<-bag_words_bbc_bs_new[,-1] %>% as.data.frame()
library(dismo)
kfold(1:nrow(banco_de_dados),k = 10)
j=1
folds==j
folds<-kfold(1:nrow(banco_de_dados),k = 10)
folds==j
setwd("D:/my_computer")
getwd()
library(knitr)
k=1
acc_rep<-numeric(0)
for(i in 1:10){
acc<-numeric(0)
for(j in 1:10){
folds<-kfold(1:nrow(banco_de_dados),k = 10)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
}
acc_rep[i]<-mean(acc)
}
library(sboost)
acc_rep<-numeric(0)
for(i in 1:10){
acc<-numeric(0)
for(j in 1:10){
folds<-kfold(1:nrow(banco_de_dados),k = 10)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
}
acc_rep[i]<-mean(acc)
}
acc
acc_rep
mean(acc_rep)
tic()
acc_rep<-numeric(0)
for(i in 1:10){
acc<-numeric(0)
for(j in 1:10){
folds<-kfold(1:nrow(banco_de_dados),k = 10)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
}
acc_rep[i]<-mean(acc)
}
library(tictoc)
tic()
acc_rep<-numeric(0)
for(i in 1:10){
acc<-numeric(0)
for(j in 1:10){
folds<-kfold(1:nrow(banco_de_dados),k = 10)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
}
acc_rep[i]<-mean(acc)
}
toc()
400/8
0.7*400
0.8*400
folds<-kfold(1:nrow(banco_de_dados),k = 6)
400/8
400/6
folds<-kfold(1:nrow(banco_de_dados),k = 5)
training_data<-banco_de_dados[folds!=j,]
j
j=1
folds<-kfold(1:nrow(banco_de_dados),k = 5)
training_data<-banco_de_dados[folds!=j,]
folds<-kfold(1:nrow(banco_de_dados),k = 4)
training_data<-banco_de_dados[folds!=j,]
folds<-kfold(1:nrow(banco_de_dados),k = 3)
training_data<-banco_de_dados[folds!=j,]
folds<-kfold(1:nrow(banco_de_dados),k = 4)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
pred_news
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
acc[j]
for(j in 1:30){
folds<-kfold(1:nrow(banco_de_dados),k = 4)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
}
tic()
acc_rep<-numeric(0)
for(j in 1:30){
folds<-kfold(1:nrow(banco_de_dados),k = 4)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
print(j)
}
toc()
tic()
acc_rep<-numeric(0)
#for(i in 1:10){
#acc<-numeric(0)
for(j in 1:30){
folds<-kfold(1:nrow(banco_de_dados),k = 4)
training_data<-banco_de_dados[folds!=j,]
test_data<-banco_de_dados[folds==j,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news<-predict(mod,test_data) %>% as.factor
acc[j]<-sum(diag(table(pred_news,test_data$news_class)))/sum(table(pred_news,test_data$news_class))
print(j)
}
#acc_rep[i]<-mean(acc)
#}
toc()
#===================
acc<-numeric(0)
j=1
folds<-kfold(1:nrow(banco_de_dados),k = 4)
folds
folds<-kfold(1:nrow(banco_de_dados),k = 4)
folds
j
#===================
acc<-numeric(0)
rm(list=ls())
setwd('D:/my_computer/Est ML 2018_complete/CSDS')
#read_csv1('bag_words_bbc_bs_new.csv')
bag_words_bbc_bs_new <- read_csv("D:/my_computer/Est ML 2018_complete/CSDS/bag_words_bbc_bs_new.csv")
bag_words_bbc_bs_new<-bag_words_bbc_bs_new%>%
mutate(news_class=news_class%>%as.factor)
banco_de_dados<-bag_words_bbc_bs_new[,-1] %>% as.data.frame()
banco_de_dados$news_class<-ifelse(banco_de_dados$news_class==2,1,-1) %>% as.factor
#===================
acc_train<-numeric(0)
acc_test<-numeric(0)
for(k in 1:500){
#for(i in 1:10){
#acc<-numeric(0)
training_index<-sample(1:nrow(banco_de_dados),round(0.7*nrow(banco_de_dados)))
training_data<-banco_de_dados[training_index,]
test_data<-banco_de_dados[-training_index,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news_test<-predict(mod,test_data) %>% as.factor
pred_news_train<-predict(mod,training_data) %>% as.factor
acc_train[k]<-sum(diag(table(pred_news_train,test_data$news_class)))/sum(table(pred_news_train,test_data$news_class))
acc_test[k]<-sum(diag(table(pred_news_test,test_data$news_class)))/sum(table(pred_news_test,test_data$news_class))
print(k)
#acc_rep[i]<-mean(acc)
#}
}
#===================
acc_train<-numeric(0)
acc_test<-numeric(0)
for(k in 1:500){
#for(i in 1:10){
#acc<-numeric(0)
training_index<-sample(1:nrow(banco_de_dados),round(0.7*nrow(banco_de_dados)))
training_data<-banco_de_dados[training_index,]
test_data<-banco_de_dados[-training_index,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news_test<-predict(mod,test_data) %>% as.factor
pred_news_train<-predict(mod,training_data) %>% as.factor
acc_train[k]<-sum(diag(table(pred_news_train,training_data$news_class)))/sum(table(pred_news_train,training_data$news_class))
acc_test[k]<-sum(diag(table(pred_news_test,test_data$news_class)))/sum(table(pred_news_test,test_data$news_class))
print(k)
#acc_rep[i]<-mean(acc)
#}
}
plot(1:500,acc_train)
plot(1:500,acc_test)
lines(1:500,acc_test)
lines(1:500,1-acc_test)
dev.off()
lines(1:500,1-acc_test)
plot(1:500,1-acc_test,lty='l')
plot(1:500,1-acc_test)
line(1:500,1-acc_test)
plot(line(1:500,1-acc_test))
line(1:500,1-acc_test)
line(1:500,1-acc_test,type='l')
plot(1:500,1-acc_test,type='l')
error_train<-1-acc_train
error_test<-1-acc_test
error_train_data<-data.frame(error=error_train,
type_data=as.factor("train"),
n_model=1:500)
error_test_data<-data.frame(error=error_test,
type_data=as.factor('test'),
n_model=1:500)
write.csv(error_train_data,'error_tr_backup.csv')
write.csv(error_test_data,'error_test_backup.csv')
error_500<-rbind(error_train_data,error_test_data)
ggplot(error_500)+
geom_line(mapping = aes(x=n_model,y=error,col=type_data),lwd=1)+
scale_y_continuous(limits = c(0.0,max(0.5,error_animation()$error)),expand=c(0,0))+
xlab("Number of Models")+
ylab("% Error")+
ggtitle("AdaBoosting Error")+
scale_color_manual(values=c("train"='#33658A',"test"='#CC6C35'),labels = c("train"="Training Error","test"="Test Error"))+
guides(col=guide_legend(title = NULL,keywidth = 2,keyheight = 1.5))+
theme_bw()+
theme(legend.justification=c(0.8,0.8), legend.position=c(0.9,0.9),
legend.text = element_text(size=13),
legend.background = element_rect(colour = 'black',
linetype = 'solid'))
ggplot(error_500)+
geom_line(mapping = aes(x=n_model,y=error,col=type_data),lwd=1)+
scale_y_continuous(limits = c(0.0,max(0.5,error_500$error)),expand=c(0,0))+
xlab("Number of Models")+
ylab("% Error")+
ggtitle("AdaBoosting Error")+
scale_color_manual(values=c("train"='#33658A',"test"='#CC6C35'),labels = c("train"="Training Error","test"="Test Error"))+
guides(col=guide_legend(title = NULL,keywidth = 2,keyheight = 1.5))+
theme_bw()+
theme(legend.justification=c(0.8,0.8), legend.position=c(0.9,0.9),
legend.text = element_text(size=13),
legend.background = element_rect(colour = 'black',
linetype = 'solid'))
ggplot(error_500)+
geom_line(mapping = aes(x=n_model,y=error,col=type_data),lwd=0.5)+
scale_y_continuous(limits = c(0.0,max(0.5,error_500$error)),expand=c(0,0))+
xlab("Number of Models")+
ylab("% Error")+
ggtitle("AdaBoosting Error")+
scale_color_manual(values=c("train"='#33658A',"test"='#CC6C35'),labels = c("train"="Training Error","test"="Test Error"))+
guides(col=guide_legend(title = NULL,keywidth = 2,keyheight = 1.5))+
theme_bw()+
theme(legend.justification=c(0.8,0.8), legend.position=c(0.9,0.9),
legend.text = element_text(size=13),
legend.background = element_rect(colour = 'black',
linetype = 'solid'))
ggplot(error_500)+
geom_line(mapping = aes(x=n_model,y=error,col=type_data),lwd=0.5)+
scale_y_continuous(limits = c(0.0,max(0.5,error_500$error)),expand=c(0,0))+
xlab("Number of Models")+
ylab("% Error")+
ggtitle("AdaBoosting Error BBC News")+
scale_color_manual(values=c("train"='#33658A',"test"='#CC6C35'),labels = c("train"="Training Error","test"="Test Error"))+
guides(col=guide_legend(title = NULL,keywidth = 2,keyheight = 1.5))+
theme_bw()+
theme(legend.justification=c(0.8,0.8), legend.position=c(0.9,0.9),
legend.text = element_text(size=13),
legend.background = element_rect(colour = 'black',
linetype = 'solid'))
#===================
acc_train<-numeric(0)
acc_test<-numeric(0)
acc_mean_train<-numeric(0)
acc_mean_test<-numeric(0)
for(k in 1:500){
for(i in 1:10){
#acc<-numeric(0)
training_index<-sample(1:nrow(banco_de_dados),round(0.7*nrow(banco_de_dados)))
training_data<-banco_de_dados[training_index,]
test_data<-banco_de_dados[-training_index,]
mod<-sboost(training_data[-5230],training_data[5230],iterations = k)
pred_news_test<-predict(mod,test_data) %>% as.factor
pred_news_train<-predict(mod,training_data) %>% as.factor
acc_train[i]<-sum(diag(table(pred_news_train,training_data$news_class)))/sum(table(pred_news_train,training_data$news_class))
acc_test[i]<-sum(diag(table(pred_news_test,test_data$news_class)))/sum(table(pred_news_test,test_data$news_class))
print(k)
}
acc_mean_train[k]<-mean(acc_train)
acc_mean_test[k]<-mean(acc_test)
}
error_mean_train<-1-acc_mean_train
error_mean_test<-1-acc_mean_test
#==========
error_train_data_mean<-data.frame(error=error_mean_train,
type_data=as.factor("train"),
n_model=1:200)
error_test_data_mean<-data.frame(error=error_mean_test,
type_data=as.factor('test'),
n_model=1:500)
error_test_data_mean<-data.frame(error=error_mean_test,
type_data=as.factor('test'),
n_model=1:200)
write.csv(error_train_data_mean,'error_tr_backup_mean.csv')
write.csv(error_test_data_mean,'error_test_backup_mean.csv')
error_200<-rbind(error_train_data_mean,error_test_data_mean)
ggplot(error_200)+
geom_line(mapping = aes(x=n_model,y=error,col=type_data),lwd=0.5)+
scale_y_continuous(limits = c(0.0,max(0.5,error_200$error)),expand=c(0,0))+
xlab("Number of Models")+
ylab("% Error")+
ggtitle("AdaBoosting Error BBC News")+
scale_color_manual(values=c("train"='#33658A',"test"='#CC6C35'),labels = c("train"="Training Error","test"="Test Error"))+
guides(col=guide_legend(title = NULL,keywidth = 2,keyheight = 1.5))+
theme_bw()+
theme(legend.justification=c(0.8,0.8), legend.position=c(0.9,0.9),
legend.text = element_text(size=13),
legend.background = element_rect(colour = 'black',
linetype = 'solid'))
which(error_mean_test)
which.min(error_mean_test)
error_mean_test[which.min(error_mean_test)]
acc_mean_test
setwd("D:/my_computer/Est_ML_2019/presentation_meet_data_bahia")
shiny::runApp('D:/my_computer/Est_ML_2019/adaboost_implementation/shiny/adaboost_project/adaBoosting')
runApp('D:/my_computer/Est_ML_2019/adaboost_implementation/shiny/adaboost_project/adaBoosting')
setwd("D:/my_computer/Est_ML_2019/adaboost_implementation/shiny/adaboost_project/adaBoosting")
runApp()
rm(list=ls())
runApp()
runApp()
runApp()
runApp()
runApp()
getwd()
runApp()
runApp()
runApp()
getwd()
setwd("E:/presentation_meet_data_bahia")
install.packages('xaringan')
library(xaringan)
install.packages('xaringan')
install.packages("xaringan")
#install.packages('xaringan')
install.packages('xaringanthemer')
# install.packages("devtools")
devtools::install_github("gadenbuie/xaringanthemer")
#install.packages('xaringan')
install.packages("devtools")
devtools::install_github("gadenbuie/xaringanthemer")
#install.packages('xaringan')
# install.packages("devtools")
#devtools::install_github("gadenbuie/xaringanthemer")
install.packages('kabbleExtra')
#install.packages('xaringan')
# install.packages("devtools")
#devtools::install_github("gadenbuie/xaringanthemer")
install.packages('kableExtra')
#install.packages('xaringan')
# install.packages("devtools")
#devtools::install_github("gadenbuie/xaringanthemer")
#install.packages('kableExtra')
install.packages("rpart.plot")
#install.packages('xaringan')
# install.packages("devtools")
#devtools::install_github("gadenbuie/xaringanthemer")
#install.packages('kableExtra')
#install.packages("rpart.plot")
install.packages(rpart)
#install.packages('xaringan')
# install.packages("devtools")
#devtools::install_github("gadenbuie/xaringanthemer")
#install.packages('kableExtra')
#install.packages("rpart.plot")
install.packages("rpart")
remotes::install_github('yihui/xaringan', upgrade = TRUE)
library(xaringan)
inf_mr()
install.packages(c("assertthat", "backports", "benchmarkmeData", "broom", "callr", "caret", "cli", "clipr", "colorspace", "curl", "data.table", "dbplyr", "devtools", "dplyr", "e1071", "forcats", "fs", "future", "future.apply", "gganimate", "ggplot2", "git2r", "gower", "gtable", "haven", "ipred", "lava", "lazyeval", "Matrix", "modelr", "openssl", "pkgbuild", "plotly", "pracma", "processx", "purrr", "quantmod", "readxl", "recipes", "registry", "remotes", "reticulate", "rpart.plot", "rstudioapi", "rvest", "sandwich", "shiny", "sys", "tibble", "tidyr", "usethis", "xtable", "zoo"))
